{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import variable\n",
    "import torchtext\n",
    "from torchtext import data,datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0-&gt;0</th>\n",
       "      <th>0-&gt;1</th>\n",
       "      <th>1-&gt;0</th>\n",
       "      <th>1-&gt;1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.627551</td>\n",
       "      <td>0.147959</td>\n",
       "      <td>0.168367</td>\n",
       "      <td>0.056122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0->0      0->1      1->0      1->1\n",
       "0  0.627551  0.147959  0.168367  0.056122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../data1_train_tree/apwsE950331.0603-0-1'\n",
    "f = open(file)\n",
    "article = []\n",
    "for sentence in f:\n",
    "    new_sentence = process(sentence)\n",
    "    article.append(new_sentence)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(article)\n",
    "vectorizer.vocabulary_\n",
    "X.toarray()\n",
    "show = pd.DataFrame(data=X.toarray(),columns=vectorizer.vocabulary_)\n",
    "show.index = ['sentence'+str(i) for i in range(1,len(show.index)+1)]\n",
    "def transition_to_vector(t_n):\n",
    "    columns = [(0,0),(0,1),(1,0),(1,1)]\n",
    "    article_vector = np.array([])\n",
    "    transitions = list()\n",
    "    for c in range(t_n.shape[1]):\n",
    "        for r in range(t_n.shape[0]-1):\n",
    "            transitions.append((t_n[r,c],t_n[r+1,c]))\n",
    "    article_counter = Counter(transitions)\n",
    "    for index in columns:\n",
    "        count = article_counter.get(index,0)\n",
    "        article_vector = np.append(article_vector,np.array([count]))\n",
    "    return (article_vector/article_vector.sum())[np.newaxis,:]\n",
    "transition_to_vector(X.toarray())\n",
    "pd.DataFrame(data=transition_to_vector(X.toarray()),columns=['0->0','0->1','1->0','1->1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2980, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../finaldataset1/earthquake_train.csv\",header=None).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model ---- Entity Grid \n",
    "<a href=\"https://people.csail.mit.edu/regina/my_papers/coherence.pdf\">Modeling Local Coherence: An Entity-Based Approach</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data I collected had already been parsed.\n",
    "<br>\n",
    "\n",
    "For example: \n",
    "\n",
    "(ROOT(S(S(NP (DT No) (NN damage))(VP (VBD was)(VP(ADVP (RB immediately))(VBN reported)(PP (IN in)(NP (NNP Manila)))(, ,)(SBAR(WHADVP (WRB where))(S(NP (JJ tall) (NNS buildings))(VP (VBD were)(VP (VBN jolted)(CC and)(VBN shaken)(PP (IN by)(NP (DT the) (NN temblor))))))))))(, ,)(CC or)(S(NP(NP (JJ other) (NNS areas))(PP (IN outside)(NP(NP (NNP Mindoro) (NNP Island))(, ,)(NP(NP (DT a) (NN police) (NN spokesman))(PP (IN in)(NP (NNP Manila)))))))(VP (VBD said)))(. .)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'damage manila buildings temblor areas mindoro island police spokesman manila'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(text):\n",
    "    '''This function will extract all of the noun in a sentence, \n",
    "    then put them together into a string seperated by white splace'''\n",
    "    result = re.findall('\\((NNP|NNPS|NNS|NN|PRP|NN) ([A-Za-z-]+)',text)\n",
    "    return ' '.join(map(lambda x : x[1].lower(),result))\n",
    "test_str = '''(ROOT(S(S(NP (DT No) (NN damage))(VP (VBD was)(VP(ADVP (RB immediately))(VBN reported)(PP (IN in)(NP (NNP Manila)))(, ,)(SBAR(WHADVP (WRB where))(S(NP (JJ tall) (NNS buildings))(VP (VBD were)(VP (VBN jolted)(CC and)(VBN shaken)(PP (IN by)(NP (DT the) (NN temblor))))))))))(, ,)(CC or)(S(NP(NP (JJ other) (NNS areas))(PP (IN outside)(NP(NP (NNP Mindoro) (NNP Island))(, ,)(NP(NP (DT a) (NN police) (NN spokesman))(PP (IN in)(NP (NNP Manila)))))))(VP (VBD said)))(. .)))'''\n",
    "process(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will transfer all of the data sets from parsed sentences to entity grid by CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths = [\"../data1_test_tree/\",\"../data1_train_tree/\",\n",
    "       \"../data2_test_tree/\",\"../data2_train_tree/\"]\n",
    "to_paths = path = [\"../data1_test_numpy/\",\"../data1_train_numpy/\",\n",
    "       \"../data2_test_numpy/\",\"../data2_train_numpy/\"]\n",
    "for path,to_path in zip(paths,to_paths):\n",
    "    for file in list(filter(lambda file: re.findall('^[a-zA-Z0-9]',file),os.listdir(path))):\n",
    "        if path in [\"../data1_test_tree/\",\"../data2_test_tree/\"]:\n",
    "            if file.endswith(\"perm-0\"):\n",
    "                f = open(path+file)\n",
    "                article = []\n",
    "                for sentence in f:\n",
    "                    new_sentence = process(sentence)\n",
    "                    article.append(new_sentence)\n",
    "                vectorizer = CountVectorizer(binary=True)\n",
    "                X = vectorizer.fit_transform(article)\n",
    "                pickle.dump(X.toarray(), open(to_path+file, \"wb\" ))\n",
    "        else : \n",
    "            f = open(path+file)\n",
    "            article = []\n",
    "            for sentence in f:\n",
    "                new_sentence = process(sentence)\n",
    "                article.append(new_sentence)\n",
    "            vectorizer = CountVectorizer(binary=True)\n",
    "            X = vectorizer.fit_transform(article)\n",
    "            pickle.dump(X.toarray(), open(to_path+file, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pickle.load(open('../data1_test_numpy/apwsE950407.0558-2-2-0.perm-0','rb'))\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each article has been transfered to an entity grid, then the following function will convert the entity grid to a vector which shows how all of the nouns are transfer between sentences within the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_to_vector(t_n):\n",
    "    columns = [(0,0),(0,1),(1,0),(1,1)]\n",
    "    article_vector = np.array([])\n",
    "    transitions = list()\n",
    "    for c in range(t_n.shape[1]):\n",
    "        for r in range(t_n.shape[0]-1):\n",
    "            transitions.append((t_n[r,c],t_n[r+1,c]))\n",
    "    article_counter = Counter(transitions)\n",
    "    for index in columns:\n",
    "        count = article_counter.get(index,0)\n",
    "        article_vector = np.append(article_vector,np.array([count]))\n",
    "    return (article_vector/article_vector.sum())[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73375262, 0.11949686, 0.12997904, 0.01677149]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_to_vector(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will generate maximum 20 permutation of the sentences from each of the original articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n",
      "same order,try again\n"
     ]
    }
   ],
   "source": [
    "paths = ['../data1_test_numpy/','../data1_train_numpy/',\n",
    "        '../data2_test_numpy/','../data2_train_numpy/']\n",
    "save_names = ['data1_test.txt','data1_train.txt','data2_test.txt','data2_train.txt']\n",
    "for path,save_name in zip(paths,save_names):\n",
    "    #path = \"./data1_test_tree_numpy/\"\n",
    "    data = None\n",
    "    for file in list(filter(lambda file: re.findall('^[a-zA-Z0-9]',file),os.listdir(path))):\n",
    "        f = open(path+file,'rb')\n",
    "        article_transition = pickle.load(f)\n",
    "        sentence_len = article_transition.shape[0]\n",
    "        origin_article_vector = transition_to_vector(article_transition)\n",
    "        origin_article_vector = np.append(origin_article_vector,np.array([[1]]),axis=1)\n",
    "        index = np.arange(sentence_len)\n",
    "        origin_index = np.arange(sentence_len)\n",
    "        origin_index = origin_index[np.newaxis,:]\n",
    "        if sentence_len < 8:\n",
    "            permutation = 10\n",
    "        else:\n",
    "            permutation = 20\n",
    "        i = 0\n",
    "        while i < permutation:\n",
    "            np.random.shuffle(index)\n",
    "            index = index[::-1]\n",
    "            np.random.shuffle(index)\n",
    "            if (((origin_index==index).sum(axis=1))== sentence_len).sum() == 1:\n",
    "                print('same order,try again')\n",
    "            else:\n",
    "                i = i+1\n",
    "                origin_index = np.vstack([origin_index,index])\n",
    "                permutation_article_transition = article_transition[index,:]\n",
    "                permutation_vector = transition_to_vector(permutation_article_transition)\n",
    "                permutation_vector = np.append(permutation_vector,np.array([[0]]),axis=1)\n",
    "                if data is None:\n",
    "                    data = np.copy(origin_article_vector)\n",
    "                else:\n",
    "                    data = np.append(data,origin_article_vector,axis=0)\n",
    "                data = np.append(data,permutation_vector,axis=0)\n",
    "    #print(data.shape)\n",
    "    pickle.dump(data, open('../finalnumpy/'+save_name, \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_test = open('../finalnumpy/data1_test.txt','rb')\n",
    "earthquake_test = pickle.load(earthquake_test)\n",
    "\n",
    "earthquake_train = open('../finalnumpy/data1_train.txt','rb')\n",
    "earthquake_train = pickle.load(earthquake_train)\n",
    "\n",
    "airplane_train = open('../finalnumpy/data2_train.txt','rb')\n",
    "airplane_train = pickle.load(airplane_train)\n",
    "\n",
    "airplane_test = open('../finalnumpy/data2_test.txt','rb')\n",
    "airplane_test = pickle.load(airplane_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train earthquake article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'kernel': ['linear', 'poly', 'rbf'], 'tol': [0.001, 0.0001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'C' : [10**i for i in range(-5,2)],\n",
    "    'kernel':[ 'linear', 'poly', 'rbf'],\n",
    "    'tol':[1e-3,1e-4]\n",
    "}\n",
    "svc = SVC(probability=True)\n",
    "clf = GridSearchCV(svc, parameters,cv=5)\n",
    "clf.fit(airplane_train[:,:-1],airplane_train[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test earthquake article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8386934673366834"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.best_estimator_.predict_proba(airplane_test[:,:-1])[:,1]\n",
    "a = pred[::2]\n",
    "b = pred[1::2]\n",
    "correct_rate = ((a>b).sum())/len(a)\n",
    "correct_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train airplane articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'kernel': ['linear', 'poly', 'rbf'], 'tol': [0.001, 0.0001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'C' : [10**i for i in range(-5,2)],\n",
    "    'kernel':[ 'linear', 'poly', 'rbf'],\n",
    "    'tol':[1e-3,1e-4]\n",
    "}\n",
    "svc = SVC(probability=True)\n",
    "clf = GridSearchCV(svc, parameters,cv=5,n_jobs=-1)\n",
    "clf.fit(earthquake_train[:,:-1],earthquake_train[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test airplace articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8034682080924855"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.best_estimator_.predict_proba(earthquake_test[:,:-1])[:,1]\n",
    "a = pred[::2]\n",
    "b = pred[1::2]\n",
    "correct_rate = ((a>b).sum())/len(a)\n",
    "correct_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> TreeRnn + GRN </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert parsed sentences to orignal sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"About 3 minutes later , he was told his destination airport was at 12 o'clock and 8 miles , which he acknowledged .\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"(ROOT(S(ADVP(NP(QP (RB About) (CD 3))(NNS minutes))(RB later))(, ,)(NP (PRP he))(VP (VBD was)(VP (VBN told)(SBAR(S(NP (PRP$ his) (NN destination) (NN airport))(VP (VBD was)(PP (IN at)(NP(NP (CD 12) (RB o'clock))(CC and)(NP(NP (CD 8) (NNS miles))(, ,)(SBAR(WHNP (WDT which))(S(NP (PRP he))(VP (VBD acknowledged))))))))))))(. .)))\"\n",
    "text = re.sub(r'\\s*(\\([A-Z,.!#|$:;\\'``\"-]+)|(\\))\\s*', '', line)\n",
    "text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"../data2_train_tree/\"\n",
    "new_path = \"../data2_train_tree_revised/\"\n",
    "for name in list(filter(lambda name: re.findall(\"^[a-zA-Z0-9]\",name),os.listdir(path))):\n",
    "    f = open(path+name)\n",
    "    w = open(new_path+name,'w')\n",
    "    for text in f:\n",
    "        text = re.sub(r'\\s*(\\([A-Z,.!#|$:;\\'``\"-]+)|(\\))\\s*', '', text)\n",
    "        text = text[1:]\n",
    "        w.write(text.strip()+\"\\n\")\n",
    "    w.close()\n",
    "    \n",
    "path = \"../data1_train_tree/\"\n",
    "new_path = \"../data1_train_tree_revised/\"\n",
    "for name in list(filter(lambda name: re.findall(\"^[a-zA-Z0-9]\",name),os.listdir(path))):\n",
    "    f = open(path+name)\n",
    "    w = open(new_path+name,'w')\n",
    "    for text in f:\n",
    "        text = re.sub(r'\\s*(\\([A-Z,.!#|$:;\\'``\"-]+)|(\\))\\s*', '', text)\n",
    "        text = text[1:]\n",
    "        w.write(text.strip()+\"\\n\")\n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data2_test_tree/\"\n",
    "new_path = \"../data2_test_tree_revised/\"\n",
    "for name in list(filter(lambda name: re.findall(\"^[a-zA-Z0-9]\",name),os.listdir(path))):\n",
    "    if re.findall(\".*.perm-0$\",name):\n",
    "        f = open(path+name)\n",
    "        w = open(new_path+name,'w')\n",
    "        for text in f:\n",
    "            text = re.sub(r'\\s*(\\([A-Z,.!#|$:;\\'``\"-]+)|(\\))\\s*', '', text)\n",
    "            text = text[1:]\n",
    "            w.write(text.strip()+\"\\n\")\n",
    "        w.close()\n",
    "    \n",
    "path = \"../data1_test_tree/\"\n",
    "new_path = \"../data1_test_tree_revised/\"\n",
    "for name in list(filter(lambda name: re.findall(\"^[a-zA-Z0-9]\",name),os.listdir(path))):\n",
    "    if re.findall(\".*.perm-0$\",name):\n",
    "        f = open(path+name)\n",
    "        w = open(new_path+name,'w')\n",
    "        for text in f:\n",
    "            text = re.sub(r'\\s*(\\([A-Z,.!#|$:;\\'``\"-]+)|(\\))\\s*', '', text)\n",
    "            text = text[1:]\n",
    "            w.write(text.strip()+\"\\n\")\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call edu.stanford.nlp.sentiment.SentimentPipeline to generate pennTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpaths = ['../data2_train_tree_revised/','../data2_test_tree_revised/',\n",
    "         '../data1_train_tree_revised/','../data1_test_tree_revised/']\n",
    "to_dirpaths = ['../data2_train_tree_binary1/','../data2_test_tree_binary1/',\n",
    "         '../data1_train_tree_binary1/','../data1_test_tree_binary1/']\n",
    "\n",
    "for dirpath,to_dirpath in zip(dirpaths,to_dirpaths):\n",
    "    filenames = (list(filter(lambda name: re.findall('^[a-z0-9A-Z].*',name),os.listdir(dirpath))))\n",
    "    for filename in filenames:\n",
    "        os.system('java -cp \"*\" edu.stanford.nlp.sentiment.SentimentPipeline -file {} -output pennTrees > {}'.format(dirpath+filename,to_dirpath+filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> example of the result </h3>\n",
    "BC-Philippines-Quake , 2nd Ld-Writethru | Strong Quake Rocks Philippines Island , 25 Deaths Reported | EDS : New thruout to UPDATE with radio report saying 25 people died , | many buildings collapsed , island still recovering from a typhoon | that struck last month .\n",
    "(0 (1 (1 (2 (2 BC-Philippines-Quake) (2 (2 ,) (2 (2 2nd) (2 Ld-Writethru)))) (1 (1 (2 |) (1 (1 (1 (2 (2 (2 (2 Strong) (2 Quake)) (2 (2 Rocks) (2 (2 Philippines) (2 Island)))) (2 ,)) (2 (2 25) (2 (2 Deaths) (2 (2 Reported) (2 (2 |) (2 EDS)))))) (2 :)) (2 (3 New) (2 thruout)))) (2 (2 to) (2 (2 UPDATE) (2 (2 with) (2 (2 (2 radio) (2 report)) (2 (2 saying) (2 (2 25) (2 people))))))))) (1 died)) (1 (2 ,) (1 (2 (2 |) (2 (2 many) (2 buildings))) (1 (1 (2 (2 collapsed) (2 ,)) (1 (2 island) (1 (2 still) (1 (2 recovering) (2 (2 from) (2 (2 (2 a) (2 (2 typhoon) (2 |))) (2 (2 that) (2 (2 struck) (2 (2 last) (3 month)))))))))) (2 .)))))\n",
    "No pickup .\n",
    "(2 (1 No) (2 (2 pickup) (2 .)))\n",
    "| By THOMAS WAGNER | Associated Press Writer .\n",
    "(1 (1 (2 |) (1 (2 By) (1 (2 THOMAS) (1 (2 WAGNER) (1 (2 |) (1 (2 Associated) (2 (2 Press) (2 Writer)))))))) (2 .))\n",
    "MANILA , Philippines -LRB- AP -RRB- A strong earthquake rocked the Philippine island of Mindoro early Tuesday , reportedly killing at least 25 people and destroying many homes .\n",
    "(1 (3 (2 (2 MANILA) (2 ,)) (3 (2 (2 Philippines) (2 (2 -LRB-) (2 (2 AP) (2 -RRB-)))) (3 (2 A) (3 (3 strong) (2 earthquake))))) (1 (1 (1 (1 (2 (2 rocked) (2 (2 (2 the) (2 (2 Philippine) (2 island))) (2 (2 of) (2 Mindoro)))) (2 (2 early) (2 Tuesday))) (2 ,)) (1 (2 reportedly) (1 (1 (1 (2 killing) (2 (2 (2 (2 at) (1 least)) (2 25)) (2 people))) (2 and)) (2 (2 destroying) (2 (2 many) (2 homes)))))) (2 .)))\n",
    "DZRH Radio , with a reporter on the scene , said most of those people died while sleeping in buildings that collapsed , including a structure that houses the Philippine Coast Guard .\n",
    "(1 (1 (1 (2 (2 (2 DZRH) (2 Radio)) (2 ,)) (2 (2 with) (2 (2 (2 a) (2 reporter)) (2 (2 on) (2 (2 the) (2 scene)))))) (2 ,)) (2 (2 (2 said) (1 (2 (2 most) (2 (2 of) (2 (2 those) (2 people)))) (1 (2 (2 (1 died) (2 (2 while) (2 (2 sleeping) (2 (2 in) (2 (2 buildings) (2 (2 that) (2 collapsed))))))) (2 ,)) (2 (2 including) (3 (2 (2 a) (2 structure)) (2 (2 that) (2 (2 houses) (2 (2 the) (2 (2 Philippine) (2 (2 Coast) (2 Guard))))))))))) (2 .)))\n",
    "Mindoro Island is still recovering from a typhoon that struck the area last month , destroying much of the coconut and rice that famrs grow in small fields there .\n",
    "(1 (2 (2 Mindoro) (2 Island)) (1 (1 (2 (2 is) (2 still)) (1 (2 recovering) (1 (2 from) (1 (2 (2 a) (2 typhoon)) (1 (2 that) (1 (2 (2 (2 (2 struck) (2 (2 the) (2 area))) (2 (2 last) (3 month))) (2 ,)) (2 (2 (2 destroying) (2 (2 much) (2 (2 of) (2 (2 the) (2 (2 (2 coconut) (2 and)) (2 rice)))))) (2 (2 that) (2 (2 famrs) (2 (2 (2 grow) (2 (2 in) (2 (2 small) (2 fields)))) (2 there))))))))))) (2 .)))\n",
    "The quake , which occurred at 3:15 am -LRB- 14:15 Monday GMT -RRB- , had a preliminary magnitude of 6.7 and was centered in a strait 10 kilometers -LRB- six miles -RRB- north of Calapan , a town on the northern end of the island , said the Philippine Institute of Vulcanology and Seismology .\n",
    "(1 (1 (1 (2 (2 (2 The) (2 quake)) (2 ,)) (1 (2 which) (1 (1 (1 (1 (2 (2 occurred) (2 (2 at) (1 (2 (2 3:15) (2 am)) (2 (2 -LRB-) (2 (2 (2 14:15) (2 (2 Monday) (2 GMT))) (2 -RRB-)))))) (2 ,)) (2 (2 had) (2 (3 (2 a) (2 (2 preliminary) (2 magnitude))) (2 (2 of) (2 6.7))))) (2 and)) (2 (2 was) (2 (2 centered) (1 (2 in) (1 (1 (2 a) (1 (2 (2 strait) (2 (3 10) (2 kilometers))) (2 (2 (2 -LRB-) (2 (2 (2 six) (2 miles)) (2 -RRB-))) (2 north)))) (2 (2 of) (3 (2 (2 Calapan) (2 ,)) (2 (2 (2 a) (3 town)) (2 (2 on) (2 (2 (2 the) (2 (2 northern) (2 end))) (2 (2 of) (2 (2 the) (2 island))))))))))))))) (2 ,)) (2 (2 (2 said) (2 (2 (2 the) (2 (2 Philippine) (2 Institute))) (2 (2 of) (2 (2 (2 Vulcanology) (2 and)) (2 Seismology))))) (2 .)))\n",
    "Lasting about four minutes , the quake was felt at intensities of 3 and 4 throughout central and south Philippines , including Manila , 75 miles -LRB- 120 kilometers -RRB- north of the epicenter , said Ted Sandoval , a specialist at the institute .\n",
    "(0 (1 (1 (1 (2 (2 Lasting) (2 (2 (2 about) (2 four)) (2 minutes))) (1 (2 ,) (1 (1 (2 (2 the) (2 quake)) (1 (2 was) (1 (1 (2 (2 (2 felt) (2 (2 at) (2 (2 intensities) (2 (2 of) (2 (2 (2 3) (2 and)) (2 4)))))) (2 (2 throughout) (2 (2 central) (2 (2 and) (2 (2 south) (2 Philippines)))))) (2 ,)) (1 (2 including) (1 (2 (2 Manila) (2 ,)) (1 (1 (2 (2 75) (2 miles)) (2 (2 -LRB-) (2 (2 (2 120) (2 kilometers)) (2 -RRB-)))) (2 (2 north) (2 (2 of) (2 (2 the) (2 epicenter)))))))))) (2 ,)))) (2 said)) (1 (1 (2 (2 Ted) (2 Sandoval)) (2 ,)) (2 (2 (2 a) (2 specialist)) (2 (2 at) (2 (2 the) (2 institute)))))) (2 .))\n",
    "No damage was immediately reported in Manila , where tall buildings were jolted and shaken by the temblor , or other areas outside Mindoro Island , a police spokesman in Manila said .\n",
    "(1 (1 (1 (1 (1 (2 (1 No) (1 damage)) (1 (2 was) (1 (2 immediately) (1 (2 (2 (2 reported) (2 (2 in) (2 Manila))) (2 ,)) (2 (2 where) (2 (2 (2 tall) (2 buildings)) (2 (2 were) (2 (2 (2 (2 jolted) (2 and)) (2 shaken)) (2 (2 by) (2 (2 the) (2 temblor))))))))))) (2 ,)) (2 or)) (2 (2 (2 (2 other) (2 areas)) (2 (2 outside) (2 (2 (2 (2 Mindoro) (2 Island)) (2 ,)) (2 (2 (2 a) (2 (2 police) (2 spokesman))) (2 (2 in) (2 Manila)))))) (2 said))) (2 .))\n",
    "The U.S. Geological Survey in Menlo Park , Calif. , put the quake 's preliminary magnitude at 7.1 .\n",
    "(1 (1 (1 (2 (2 The) (2 (2 U.S.) (2 (2 Geological) (2 Survey)))) (1 (2 in) (1 (1 (2 (2 (2 (2 Menlo) (2 Park)) (2 ,)) (2 Calif.)) (2 ,)) (2 put)))) (2 (2 (2 (2 the) (2 (2 quake) (2 's))) (2 (2 preliminary) (2 magnitude))) (2 (2 at) (2 7.1)))) (2 .))\n",
    "Before phone lines were cut off on Mindoro Island , Gov. Rodolfo Valencia of Oriental Mindoro province told DZBB and DZMM radio that at least six people died when their homes collapsed or were washed away by large waves , and three bridges were damaged and closed .\n",
    "(1 (2 (2 Before) (2 (2 (2 phone) (2 lines)) (2 (2 were) (2 (2 (2 cut) (1 off)) (2 (2 on) (2 (2 Mindoro) (2 Island))))))) (1 (2 ,) (1 (1 (1 (1 (1 (1 (2 (2 Gov.) (2 (2 Rodolfo) (2 Valencia))) (2 (2 of) (2 (2 Oriental) (2 (2 Mindoro) (2 province))))) (2 (2 told) (2 (2 DZBB) (2 (2 and) (2 (2 DZMM) (2 (2 radio) (1 (2 that) (1 (2 (2 (2 (2 at) (1 least)) (2 six)) (2 people)) (2 (1 died) (2 (2 when) (2 (2 (2 their) (2 homes)) (1 (2 (2 collapsed) (2 or)) (1 (2 were) (3 (3 (2 washed) (2 away)) (2 (2 by) (2 (2 large) (2 waves))))))))))))))))) (2 ,)) (2 and)) (2 (2 (2 three) (2 bridges)) (2 (2 were) (2 (2 (1 damaged) (2 and)) (2 closed))))) (2 .))))\n",
    "The DZRH reporter called Manila on a cellular phone .\n",
    "(1 (2 (2 The) (2 (2 DZRH) (2 reporter))) (2 (2 (2 called) (2 (2 Manila) (2 (2 on) (2 (2 a) (2 (2 cellular) (2 phone)))))) (2 .)))\n",
    "But Sandoval said no tidal waves were reported or expected from the quake , one of the strongest ever to occur on the Lubang fault , one of the area 's most active .\n",
    "(1 (1 (2 But) (2 (2 Sandoval) (1 (2 said) (1 (2 (1 no) (2 (3 tidal) (2 waves))) (2 (2 were) (2 (3 (2 (2 (2 reported) (2 or)) (1 expected)) (3 (2 from) (3 (2 (2 (2 the) (2 quake)) (2 ,)) (3 (2 one) (3 (2 of) (3 (3 (2 the) (3 strongest)) (2 ever))))))) (2 (2 to) (2 (2 occur) (2 (2 on) (2 (2 the) (1 (2 Lubang) (2 fault)))))))))))) (2 (2 ,) (3 (2 (2 one) (2 (2 of) (2 (2 the) (2 area)))) (2 (2 (2 's) (2 (2 most) (2 active))) (2 .)))))\n",
    "A magnitude 6 quake can cause severe damage if centered under a populated area , while amgnitude 7 quake indicates a major quake capable of widespread , heavy damage .\n",
    "(1 (2 (2 A) (2 (2 magnitude) (2 (2 6) (2 quake)))) (1 (1 (2 can) (1 (1 (2 cause) (2 (2 severe) (1 damage))) (1 (2 if) (1 (2 (2 (2 centered) (2 (2 under) (2 (2 a) (3 (2 populated) (2 area))))) (2 ,)) (1 (2 while) (1 (2 (2 amgnitude) (2 (2 7) (2 quake))) (2 (2 indicates) (1 (2 (2 a) (2 (2 major) (2 quake))) (1 (3 capable) (2 (2 of) (2 (2 widespread) (2 (2 ,) (2 (2 heavy) (1 damage)))))))))))))) (2 .)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the format of parsed sentences to the format that is more suitable to bulid recursive nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''BC-Philippines-Quake , 2nd Ld-Writethru | Strong Quake Rocks Philippines Island , 25 Deaths Reported | EDS : New thruout to UPDATE with radio report saying 25 people died , | many buildings collapsed , island still recovering from a typhoon | that struck last month .\n",
    "(0 (1 (1 (2 (2 BC-Philippines-Quake) (2 (2 ,) (2 (2 2nd) (2 Ld-Writethru)))) (1 (1 (2 |) (1 (1 (1 (2 (2 (2 (2 Strong) (2 Quake)) (2 (2 Rocks) (2 (2 Philippines) (2 Island)))) (2 ,)) (2 (2 25) (2 (2 Deaths) (2 (2 Reported) (2 (2 |) (2 EDS)))))) (2 :)) (2 (3 New) (2 thruout)))) (2 (2 to) (2 (2 UPDATE) (2 (2 with) (2 (2 (2 radio) (2 report)) (2 (2 saying) (2 (2 25) (2 people))))))))) (1 died)) (1 (2 ,) (1 (2 (2 |) (2 (2 many) (2 buildings))) (1 (1 (2 (2 collapsed) (2 ,)) (1 (2 island) (1 (2 still) (1 (2 recovering) (2 (2 from) (2 (2 (2 a) (2 (2 typhoon) (2 |))) (2 (2 that) (2 (2 struck) (2 (2 last) (3 month)))))))))) (2 .)))))\n",
    "No pickup .\n",
    "(2 (1 No) (2 (2 pickup) (2 .)))\n",
    "| By THOMAS WAGNER | Associated Press Writer .\n",
    "(1 (1 (2 |) (1 (2 By) (1 (2 THOMAS) (1 (2 WAGNER) (1 (2 |) (1 (2 Associated) (2 (2 Press) (2 Writer)))))))) (2 .))\n",
    "MANILA , Philippines -LRB- AP -RRB- A strong earthquake rocked the Philippine island of Mindoro early Tuesday , reportedly killing at least 25 people and destroying many homes .\n",
    "(1 (3 (2 (2 MANILA) (2 ,)) (3 (2 (2 Philippines) (2 (2 -LRB-) (2 (2 AP) (2 -RRB-)))) (3 (2 A) (3 (3 strong) (2 earthquake))))) (1 (1 (1 (1 (2 (2 rocked) (2 (2 (2 the) (2 (2 Philippine) (2 island))) (2 (2 of) (2 Mindoro)))) (2 (2 early) (2 Tuesday))) (2 ,)) (1 (2 reportedly) (1 (1 (1 (2 killing) (2 (2 (2 (2 at) (1 least)) (2 25)) (2 people))) (2 and)) (2 (2 destroying) (2 (2 many) (2 homes)))))) (2 .)))\n",
    "DZRH Radio , with a reporter on the scene , said most of those people died while sleeping in buildings that collapsed , including a structure that houses the Philippine Coast Guard .\n",
    "(1 (1 (1 (2 (2 (2 DZRH) (2 Radio)) (2 ,)) (2 (2 with) (2 (2 (2 a) (2 reporter)) (2 (2 on) (2 (2 the) (2 scene)))))) (2 ,)) (2 (2 (2 said) (1 (2 (2 most) (2 (2 of) (2 (2 those) (2 people)))) (1 (2 (2 (1 died) (2 (2 while) (2 (2 sleeping) (2 (2 in) (2 (2 buildings) (2 (2 that) (2 collapsed))))))) (2 ,)) (2 (2 including) (3 (2 (2 a) (2 structure)) (2 (2 that) (2 (2 houses) (2 (2 the) (2 (2 Philippine) (2 (2 Coast) (2 Guard))))))))))) (2 .)))\n",
    "Mindoro Island is still recovering from a typhoon that struck the area last month , destroying much of the coconut and rice that famrs grow in small fields there .\n",
    "(1 (2 (2 Mindoro) (2 Island)) (1 (1 (2 (2 is) (2 still)) (1 (2 recovering) (1 (2 from) (1 (2 (2 a) (2 typhoon)) (1 (2 that) (1 (2 (2 (2 (2 struck) (2 (2 the) (2 area))) (2 (2 last) (3 month))) (2 ,)) (2 (2 (2 destroying) (2 (2 much) (2 (2 of) (2 (2 the) (2 (2 (2 coconut) (2 and)) (2 rice)))))) (2 (2 that) (2 (2 famrs) (2 (2 (2 grow) (2 (2 in) (2 (2 small) (2 fields)))) (2 there))))))))))) (2 .)))\n",
    "The quake , which occurred at 3:15 am -LRB- 14:15 Monday GMT -RRB- , had a preliminary magnitude of 6.7 and was centered in a strait 10 kilometers -LRB- six miles -RRB- north of Calapan , a town on the northern end of the island , said the Philippine Institute of Vulcanology and Seismology .\n",
    "(1 (1 (1 (2 (2 (2 The) (2 quake)) (2 ,)) (1 (2 which) (1 (1 (1 (1 (2 (2 occurred) (2 (2 at) (1 (2 (2 3:15) (2 am)) (2 (2 -LRB-) (2 (2 (2 14:15) (2 (2 Monday) (2 GMT))) (2 -RRB-)))))) (2 ,)) (2 (2 had) (2 (3 (2 a) (2 (2 preliminary) (2 magnitude))) (2 (2 of) (2 6.7))))) (2 and)) (2 (2 was) (2 (2 centered) (1 (2 in) (1 (1 (2 a) (1 (2 (2 strait) (2 (3 10) (2 kilometers))) (2 (2 (2 -LRB-) (2 (2 (2 six) (2 miles)) (2 -RRB-))) (2 north)))) (2 (2 of) (3 (2 (2 Calapan) (2 ,)) (2 (2 (2 a) (3 town)) (2 (2 on) (2 (2 (2 the) (2 (2 northern) (2 end))) (2 (2 of) (2 (2 the) (2 island))))))))))))))) (2 ,)) (2 (2 (2 said) (2 (2 (2 the) (2 (2 Philippine) (2 Institute))) (2 (2 of) (2 (2 (2 Vulcanology) (2 and)) (2 Seismology))))) (2 .)))\n",
    "Lasting about four minutes , the quake was felt at intensities of 3 and 4 throughout central and south Philippines , including Manila , 75 miles -LRB- 120 kilometers -RRB- north of the epicenter , said Ted Sandoval , a specialist at the institute .\n",
    "(0 (1 (1 (1 (2 (2 Lasting) (2 (2 (2 about) (2 four)) (2 minutes))) (1 (2 ,) (1 (1 (2 (2 the) (2 quake)) (1 (2 was) (1 (1 (2 (2 (2 felt) (2 (2 at) (2 (2 intensities) (2 (2 of) (2 (2 (2 3) (2 and)) (2 4)))))) (2 (2 throughout) (2 (2 central) (2 (2 and) (2 (2 south) (2 Philippines)))))) (2 ,)) (1 (2 including) (1 (2 (2 Manila) (2 ,)) (1 (1 (2 (2 75) (2 miles)) (2 (2 -LRB-) (2 (2 (2 120) (2 kilometers)) (2 -RRB-)))) (2 (2 north) (2 (2 of) (2 (2 the) (2 epicenter)))))))))) (2 ,)))) (2 said)) (1 (1 (2 (2 Ted) (2 Sandoval)) (2 ,)) (2 (2 (2 a) (2 specialist)) (2 (2 at) (2 (2 the) (2 institute)))))) (2 .))\n",
    "No damage was immediately reported in Manila , where tall buildings were jolted and shaken by the temblor , or other areas outside Mindoro Island , a police spokesman in Manila said .\n",
    "(1 (1 (1 (1 (1 (2 (1 No) (1 damage)) (1 (2 was) (1 (2 immediately) (1 (2 (2 (2 reported) (2 (2 in) (2 Manila))) (2 ,)) (2 (2 where) (2 (2 (2 tall) (2 buildings)) (2 (2 were) (2 (2 (2 (2 jolted) (2 and)) (2 shaken)) (2 (2 by) (2 (2 the) (2 temblor))))))))))) (2 ,)) (2 or)) (2 (2 (2 (2 other) (2 areas)) (2 (2 outside) (2 (2 (2 (2 Mindoro) (2 Island)) (2 ,)) (2 (2 (2 a) (2 (2 police) (2 spokesman))) (2 (2 in) (2 Manila)))))) (2 said))) (2 .))\n",
    "The U.S. Geological Survey in Menlo Park , Calif. , put the quake 's preliminary magnitude at 7.1 .\n",
    "(1 (1 (1 (2 (2 The) (2 (2 U.S.) (2 (2 Geological) (2 Survey)))) (1 (2 in) (1 (1 (2 (2 (2 (2 Menlo) (2 Park)) (2 ,)) (2 Calif.)) (2 ,)) (2 put)))) (2 (2 (2 (2 the) (2 (2 quake) (2 's))) (2 (2 preliminary) (2 magnitude))) (2 (2 at) (2 7.1)))) (2 .))\n",
    "Before phone lines were cut off on Mindoro Island , Gov. Rodolfo Valencia of Oriental Mindoro province told DZBB and DZMM radio that at least six people died when their homes collapsed or were washed away by large waves , and three bridges were damaged and closed .\n",
    "(1 (2 (2 Before) (2 (2 (2 phone) (2 lines)) (2 (2 were) (2 (2 (2 cut) (1 off)) (2 (2 on) (2 (2 Mindoro) (2 Island))))))) (1 (2 ,) (1 (1 (1 (1 (1 (1 (2 (2 Gov.) (2 (2 Rodolfo) (2 Valencia))) (2 (2 of) (2 (2 Oriental) (2 (2 Mindoro) (2 province))))) (2 (2 told) (2 (2 DZBB) (2 (2 and) (2 (2 DZMM) (2 (2 radio) (1 (2 that) (1 (2 (2 (2 (2 at) (1 least)) (2 six)) (2 people)) (2 (1 died) (2 (2 when) (2 (2 (2 their) (2 homes)) (1 (2 (2 collapsed) (2 or)) (1 (2 were) (3 (3 (2 washed) (2 away)) (2 (2 by) (2 (2 large) (2 waves))))))))))))))))) (2 ,)) (2 and)) (2 (2 (2 three) (2 bridges)) (2 (2 were) (2 (2 (1 damaged) (2 and)) (2 closed))))) (2 .))))\n",
    "The DZRH reporter called Manila on a cellular phone .\n",
    "(1 (2 (2 The) (2 (2 DZRH) (2 reporter))) (2 (2 (2 called) (2 (2 Manila) (2 (2 on) (2 (2 a) (2 (2 cellular) (2 phone)))))) (2 .)))\n",
    "But Sandoval said no tidal waves were reported or expected from the quake , one of the strongest ever to occur on the Lubang fault , one of the area 's most active .\n",
    "(1 (1 (2 But) (2 (2 Sandoval) (1 (2 said) (1 (2 (1 no) (2 (3 tidal) (2 waves))) (2 (2 were) (2 (3 (2 (2 (2 reported) (2 or)) (1 expected)) (3 (2 from) (3 (2 (2 (2 the) (2 quake)) (2 ,)) (3 (2 one) (3 (2 of) (3 (3 (2 the) (3 strongest)) (2 ever))))))) (2 (2 to) (2 (2 occur) (2 (2 on) (2 (2 the) (1 (2 Lubang) (2 fault)))))))))))) (2 (2 ,) (3 (2 (2 one) (2 (2 of) (2 (2 the) (2 area)))) (2 (2 (2 's) (2 (2 most) (2 active))) (2 .)))))\n",
    "A magnitude 6 quake can cause severe damage if centered under a populated area , while amgnitude 7 quake indicates a major quake capable of widespread , heavy damage .\n",
    "(1 (2 (2 A) (2 (2 magnitude) (2 (2 6) (2 quake)))) (1 (1 (2 can) (1 (1 (2 cause) (2 (2 severe) (1 damage))) (1 (2 if) (1 (2 (2 (2 centered) (2 (2 under) (2 (2 a) (3 (2 populated) (2 area))))) (2 ,)) (1 (2 while) (1 (2 (2 amgnitude) (2 (2 7) (2 quake))) (2 (2 indicates) (1 (2 (2 a) (2 (2 major) (2 quake))) (1 (3 capable) (2 (2 of) (2 (2 widespread) (2 (2 ,) (2 (2 heavy) (1 damage)))))))))))))) (2 .)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_refine(text):\n",
    "    text = re.sub('\\([0-9] ',  '(', text)\n",
    "    def t(i):\n",
    "        i = i.group(0)\n",
    "        i = list(i)\n",
    "        i.remove('(')\n",
    "        i.remove(')')\n",
    "        i = ''.join(i)\n",
    "        return i\n",
    "    go = True\n",
    "    while go:\n",
    "        original = len(text)\n",
    "        text = re.sub(r'\\s*\\([^ \\)\\(\\t\\n\\r\\f\\v]+\\)\\s*', t, text)\n",
    "        go = not (original == len(text))\n",
    "    return text\n",
    "def extract_tree(text):\n",
    "    binary_tree = ''\n",
    "    count = 0\n",
    "    i = text.find('(')\n",
    "    while i < (len(text)):\n",
    "        if text[i] == '(':\n",
    "            if count == 0:\n",
    "                start = i\n",
    "            count = count +1\n",
    "        elif text[i] == ')':\n",
    "            count = count - 1\n",
    "            if count == 0:\n",
    "                end = i\n",
    "                target = (text[start:end+1])\n",
    "                if (len(target) > 5) and (list(target).count(')')>=2):\n",
    "                     binary_tree = binary_tree + text_refine(target) + '\\n'\n",
    "        i = i+1\n",
    "    return binary_tree.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((((BC-Philippines-Quake (, (2nd Ld-Writethru))) ((| ((((((Strong Quake) (Rocks (Philippines Island))) ,) (25 (Deaths (Reported (| EDS))))) :) (New thruout))) (to (UPDATE (with ((radio report) (saying (25 people)))))))) died) (, ((| (many buildings)) (((collapsed ,) (island (still (recovering (from ((a (typhoon |)) (that (struck (last month))))))))) .))))\n",
      "(No (pickup .))\n",
      "((| (By (THOMAS (WAGNER (| (Associated (Press Writer))))))) .)\n",
      "(((MANILA ,) ((Philippines (-LRB- (AP -RRB-))) (A (strong earthquake)))) (((((rocked ((the (Philippine island)) (of Mindoro))) (early Tuesday)) ,) (reportedly (((killing (((at least) 25) people)) and) (destroying (many homes))))) .))\n",
      "(((((DZRH Radio) ,) (with ((a reporter) (on (the scene))))) ,) ((said ((most (of (those people))) (((died (while (sleeping (in (buildings (that collapsed)))))) ,) (including ((a structure) (that (houses (the (Philippine (Coast Guard)))))))))) .))\n",
      "((Mindoro Island) (((is still) (recovering (from ((a typhoon) (that ((((struck (the area)) (last month)) ,) ((destroying (much (of (the ((coconut and) rice))))) (that (famrs ((grow (in (small fields))) there)))))))))) .))\n",
      "(((((The quake) ,) (which (((((occurred (at ((3:15 am) (-LRB- ((14:15 (Monday GMT)) -RRB-))))) ,) (had ((a (preliminary magnitude)) (of 6.7)))) and) (was (centered (in ((a ((strait (10 kilometers)) ((-LRB- ((six miles) -RRB-)) north))) (of ((Calapan ,) ((a town) (on ((the (northern end)) (of (the island)))))))))))))) ,) ((said ((the (Philippine Institute)) (of ((Vulcanology and) Seismology)))) .))\n",
      "(((((Lasting ((about four) minutes)) (, (((the quake) (was ((((felt (at (intensities (of ((3 and) 4))))) (throughout (central (and (south Philippines))))) ,) (including ((Manila ,) (((75 miles) (-LRB- ((120 kilometers) -RRB-))) (north (of (the epicenter))))))))) ,))) said) (((Ted Sandoval) ,) ((a specialist) (at (the institute))))) .)\n",
      "((((((No damage) (was (immediately (((reported (in Manila)) ,) (where ((tall buildings) (were (((jolted and) shaken) (by (the temblor)))))))))) ,) or) (((other areas) (outside (((Mindoro Island) ,) ((a (police spokesman)) (in Manila))))) said)) .)\n",
      "((((The (U.S. (Geological Survey))) (in (((((Menlo Park) ,) Calif.) ,) put))) (((the (quake 's)) (preliminary magnitude)) (at 7.1))) .)\n",
      "((Before ((phone lines) (were ((cut off) (on (Mindoro Island)))))) (, (((((((Gov. (Rodolfo Valencia)) (of (Oriental (Mindoro province)))) (told (DZBB (and (DZMM (radio (that ((((at least) six) people) (died (when ((their homes) ((collapsed or) (were ((washed away) (by (large waves)))))))))))))))) ,) and) ((three bridges) (were ((damaged and) closed)))) .)))\n",
      "((The (DZRH reporter)) ((called (Manila (on (a (cellular phone))))) .))\n",
      "((But (Sandoval (said ((no (tidal waves)) (were ((((reported or) expected) (from (((the quake) ,) (one (of ((the strongest) ever)))))) (to (occur (on (the (Lubang fault))))))))))) (, ((one (of (the area))) (('s (most active)) .))))\n",
      "((A (magnitude (6 quake))) ((can ((cause (severe damage)) (if (((centered (under (a (populated area)))) ,) (while ((amgnitude (7 quake)) (indicates ((a (major quake)) (capable (of (widespread (, (heavy damage))))))))))))) .))\n"
     ]
    }
   ],
   "source": [
    "print(extract_tree(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dirpaths = ['../data2_train_tree_binary1/','../data2_test_tree_binary1/',\n",
    "         '../data1_train_tree_binary1/','../data1_test_tree_binary1/']\n",
    "new_paths = ['../data2_train_tree_binary/','../data2_test_tree_binary/',\n",
    "         '../data1_train_tree_binary/','../data1_test_tree_binary/']\n",
    "\n",
    "for path,new_path in zip(to_dirpaths,new_paths):\n",
    "    for name in list(filter(lambda name: re.findall(\"^[a-zA-Z0-9]\",name),os.listdir(path))):\n",
    "        f = open(path+name).read().strip()\n",
    "        w = open(new_path+name,'w')\n",
    "        w.write(extract_tree(f))\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary for both train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train = pd.DataFrame(columns=['text'])\n",
    "paths = ['./data1_test_tree_revised/','./data1_train_tree_revised/',\n",
    "        './data2_test_tree_revised/','./data2_train_tree_revised/']\n",
    "for path in paths:\n",
    "    for name in list(filter(lambda name: re.findall(\"^[a-zA-Z0-9]\",name),os.listdir(path))):\n",
    "        text = open(path+name)\n",
    "        text = text.read()\n",
    "        test_and_train = test_and_train.append({'text':text.strip()},ignore_index=True)\n",
    "test_and_train.to_csv('all_text.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_n(x):\n",
    "    return x.split('\\n')\n",
    "#TEXT = data.Field(lower=True)\n",
    "#NEST = data.NestedField(nesting_field=TEXT,tokenize=split_n)\n",
    "#test_and_train = data.TabularDataset(path='./all_text.csv',format='csv',fields=[('nest', NEST)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save index_to_voca, voca_to_index, and glove embedding in local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEST.build_vocab(test_and_train)\n",
    "NEST.vocab.load_vectors('glove.42B.300d')\n",
    "itos = open('./vocabulary/itos.pickle','wb')\n",
    "stoi = open('./vocabulary/stoi.pickle','wb')\n",
    "pickle.dump(NEST.vocab.itos,itos)\n",
    "pickle.dump(NEST.vocab.stoi,stoi)\n",
    "torch.save(NEST.vocab.vectors,'./vocabulary/embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 20 random permutations for each article in bothing training and testing dataset, save the generated data into csv with the format of (text,binary tree, coherence), where 'text' is the plain text, 'binary tree' is corresponding binary parse tree, 'coherence' denote whether the order of the sentences is the original order or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_train =  pd.DataFrame(columns=['binary tree','coherence'])\n",
    "earthquake_test = pd.DataFrame(columns=['binary tree','coherence'])\n",
    "airplane_train =  pd.DataFrame(columns=['binary tree','coherence'])\n",
    "airplane_test = pd.DataFrame(columns=['binary tree','coherence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_20_permutation_and_save_csv(plain_text_paths,binary_tree_paths,dfs,savefiles):\n",
    "    for plain_text_path,binary_tree_path,df,savefile in zip(plain_text_paths,binary_tree_paths,dfs,savefiles):\n",
    "        for name in list(filter(lambda name: re.findall(\"^[a-zA-Z0-9]\",name),os.listdir(binary_tree_path))):\n",
    "            #plain_texts = open(plain_text_path+name)\n",
    "            binary_trees = open(binary_tree_path+name)\n",
    "            #orignial_texts = plain_texts = plain_texts.read().strip()\n",
    "            orignial_trees = binary_trees = binary_trees.read().strip()\n",
    "            #plain_texts = plain_texts.split('\\n')\n",
    "            binary_trees = binary_trees.split('\\n')\n",
    "            s_len = len(binary_trees)\n",
    "            indices = np.arange(s_len)\n",
    "            generated_indices = np.arange(s_len)[np.newaxis,:]\n",
    "            time = 0\n",
    "            if s_len < 8:\n",
    "                permutation = 10\n",
    "            else:\n",
    "                permutation = 20\n",
    "            while time<permutation:\n",
    "                np.random.shuffle(indices)\n",
    "                indices = indices[::-1]\n",
    "                np.random.shuffle(indices)\n",
    "                if (((generated_indices == indices).sum(axis=1)) == s_len).sum() == 1:\n",
    "                    print('same order, try again')\n",
    "                else:\n",
    "                    df = df.append({'binary tree':orignial_trees.strip(),'coherence':1},ignore_index=True)\n",
    "                    generated_indices = np.vstack([generated_indices,indices])\n",
    "                    #new_text = ''\n",
    "                    new_binary_tree = ''\n",
    "                    for index in indices:\n",
    "                        #new_text = new_text + plain_texts[index] + '\\n'\n",
    "                        new_binary_tree = new_binary_tree + binary_trees[index] + '\\n'\n",
    "                    df = df.append({'binary tree':new_binary_tree.strip(),'coherence':0},ignore_index=True)\n",
    "                    time = time + 1\n",
    "        df.to_csv(savefile,header=False,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n",
      "same order, try again\n"
     ]
    }
   ],
   "source": [
    "plain_text_paths = ['../data1_test_tree_revised/','../data1_train_tree_revised/',\n",
    "                   '../data2_test_tree_revised/','../data2_train_tree_revised/']\n",
    "binary_tree_paths = ['../data1_test_tree_binary/','../data1_train_tree_binary/',\n",
    "                    '../data2_test_tree_binary/','../data2_train_tree_binary/']\n",
    "dfs = [earthquake_test,earthquake_train,airplane_test,airplane_train]\n",
    "savefiles = ['../finaldataset1/earthquake_test.csv','../finaldataset1/earthquake_train.csv',\n",
    "            '../finaldataset1/airplane_test.csv','../finaldataset1/airplane_train.csv']\n",
    "generate_20_permutation_and_save_csv(plain_text_paths,binary_tree_paths,dfs,savefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    result = re.sub('\\(','',x)\n",
    "    result = re.sub('\\)',' )',result)\n",
    "    result = result.split()\n",
    "    return list(map(lambda token: token.strip(),result))\n",
    "def split_n(text):\n",
    "    return text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeRnnAndGRU(nn.Module):\n",
    "    def __init__(self,p1=0.2):\n",
    "        super().__init__()\n",
    "        stoi = open('../vocabulary/stoi.pickle','rb')\n",
    "        self.stoi = pickle.load(stoi)\n",
    "        embedding = torch.load('../vocabulary/embedding')\n",
    "        self.embedding = embedding.cuda()\n",
    "        h_0 = torch.zeros(1,1,300)\n",
    "        h_0 = torch.normal(h_0,std=0.0001)\n",
    "        self.h_0 = nn.Parameter(h_0,requires_grad=True)\n",
    "        self.linear = nn.Linear(600,300)\n",
    "        self.drop = nn.Dropout(p1)\n",
    "        self.gru = nn.GRU(input_size=300,hidden_size=300,num_layers=1,bias=True,batch_first=False)\n",
    "        self.prob = nn.Linear(300,1)\n",
    "    def forward(self,example):\n",
    "        article_vector = None\n",
    "        for sentence in example.nest_TREE:\n",
    "            stack = list()\n",
    "            for token in (sentence):\n",
    "                if token != ')':\n",
    "                    stack.append(self.embedding[self.stoi[token]].view(-1,300))\n",
    "                else:\n",
    "                    a1 = stack.pop()\n",
    "                    a2 = stack.pop()\n",
    "                    a = torch.cat((a1,a2),dim=1)\n",
    "                    a = self.linear(a)\n",
    "                    a = self.drop(a)\n",
    "                    stack.append(nn.functional.tanh(a))\n",
    "            sentence_vector = stack.pop()\n",
    "            if article_vector is None:\n",
    "                article_vector = sentence_vector.clone()\n",
    "            else:\n",
    "                article_vector = torch.cat((article_vector,sentence_vector),dim=0)\n",
    "                \n",
    "        seq,input_size = article_vector.shape\n",
    "        output,h_n = self.gru(article_vector.view(seq,1,input_size),self.h_0)\n",
    "        h_n = h_n.view(-1)\n",
    "        out = self.prob(h_n)\n",
    "        return nn.functional.sigmoid(out)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def fit(epoch,model,examples,group = 100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    indices = np.arange(len(examples))\n",
    "    np.random.shuffle(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[::-1]\n",
    "    indices = np.array_split(indices,group)\n",
    "    \n",
    "    for index in indices:\n",
    "        loss = torch.FloatTensor([0]).cuda()\n",
    "        for e in index:\n",
    "            example = examples[e]\n",
    "            target = torch.FloatTensor([int(example.target)]).cuda()\n",
    "            prob = model(example)\n",
    "            loss += nn.functional.binary_cross_entropy(prob,target,size_average=False)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        loss = (1/len(index))*loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch+1,'th training end')\n",
    "    return\n",
    "\n",
    "    \n",
    "def pred(epoch,model,examples):\n",
    "    model.eval()\n",
    "    pred = np.array([])\n",
    "    for example in examples:\n",
    "        prob = model(example)\n",
    "        target = np.array([int(example.target)])\n",
    "        prob = np.array([prob.item()])\n",
    "        pred = np.append(pred,prob)\n",
    "    a = pred[::2]\n",
    "    b = pred[1::2]\n",
    "    correct_rate = ((a>b).sum())/len(a)\n",
    "    print(epoch+1,'th test accuracuy:',correct_rate)\n",
    "    return\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train+Test on earthquake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs of articles we have for the training set: 1490.0\n",
      "The number of pairs of articles we have for the testing set: 1720.0\n",
      "1 th training end\n",
      "1 th test accuracuy: 0.8453488372093023\n",
      "2 th training end\n",
      "2 th test accuracuy: 0.9877906976744186\n",
      "3 th training end\n",
      "3 th test accuracuy: 0.9953488372093023\n",
      "4 th training end\n",
      "4 th test accuracuy: 0.9936046511627907\n",
      "5 th training end\n",
      "5 th test accuracuy: 0.9843023255813953\n",
      "6 th training end\n",
      "6 th test accuracuy: 0.9965116279069768\n",
      "7 th training end\n",
      "7 th test accuracuy: 0.9941860465116279\n",
      "8 th training end\n",
      "8 th test accuracuy: 0.997093023255814\n",
      "9 th training end\n",
      "9 th test accuracuy: 0.9906976744186047\n",
      "10 th training end\n",
      "10 th test accuracuy: 0.9953488372093023\n",
      "11 th training end\n",
      "11 th test accuracuy: 0.9930232558139535\n",
      "12 th training end\n",
      "12 th test accuracuy: 0.9947674418604651\n",
      "13 th training end\n",
      "13 th test accuracuy: 0.9808139534883721\n",
      "14 th training end\n",
      "14 th test accuracuy: 0.9959302325581395\n",
      "15 th training end\n",
      "15 th test accuracuy: 0.997093023255814\n",
      "16 th training end\n",
      "16 th test accuracuy: 0.9872093023255814\n",
      "17 th training end\n",
      "17 th test accuracuy: 0.7965116279069767\n",
      "18 th training end\n",
      "18 th test accuracuy: 0.9354651162790698\n",
      "19 th training end\n",
      "19 th test accuracuy: 0.9848837209302326\n",
      "20 th training end\n",
      "20 th test accuracuy: 0.9773255813953489\n"
     ]
    }
   ],
   "source": [
    "#TEXT = data.Field(sequential=True,lower=True,use_vocab=True)\n",
    "TREE = data.Field(sequential=False,use_vocab=False,preprocessing=fun,lower=True)\n",
    "#NEST_TEXT = data.NestedField(TEXT,tokenize=split_n)\n",
    "NEST_TREE = data.NestedField(TREE,tokenize=split_n,use_vocab=False)\n",
    "TARGET = data.Field(sequential=False)\n",
    "earthquake_train = data.TabularDataset(path='../finaldataset1/earthquake_train.csv',format='csv',fields=[('nest_TREE',NEST_TREE),\n",
    "                                                                   ('target',TARGET)])\n",
    "earthquake_test = data.TabularDataset(path='../finaldataset1/earthquake_test.csv',format='csv',fields=[('nest_TREE',NEST_TREE),\n",
    "                                                                   ('target',TARGET)])\n",
    "\n",
    "print('The number of pairs of articles we have for the training set:', len(earthquake_train.examples)/2)\n",
    "print('The number of pairs of articles we have for the testing set:' ,len(earthquake_test.examples)/2)\n",
    "\n",
    "treeRnnAndGru = TreeRnnAndGRU(p1=0.5)\n",
    "treeRnnAndGru = treeRnnAndGru.cuda()\n",
    "optimizer = torch.optim.Adam(treeRnnAndGru.parameters(),lr=0.005,weight_decay=0)\n",
    "for epoch in range(20):\n",
    "    fit(epoch,treeRnnAndGru,earthquake_train.examples)\n",
    "    pred(epoch,treeRnnAndGru,earthquake_test.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train+Test on airplane dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs of articles we have for the training set: 1960.0\n",
      "The number of pairs of articles we have for the testing set: 1990.0\n",
      "1 th training end\n",
      "1 th test accuracuy: 0.8683417085427135\n",
      "2 th training end\n",
      "2 th test accuracuy: 0.8547738693467337\n",
      "3 th training end\n",
      "3 th test accuracuy: 0.8648241206030151\n",
      "4 th training end\n",
      "4 th test accuracuy: 0.8824120603015075\n",
      "5 th training end\n",
      "5 th test accuracuy: 0.878391959798995\n",
      "6 th training end\n",
      "6 th test accuracuy: 0.885929648241206\n",
      "7 th training end\n",
      "7 th test accuracuy: 0.8577889447236181\n",
      "8 th training end\n",
      "8 th test accuracuy: 0.8899497487437186\n",
      "9 th training end\n",
      "9 th test accuracuy: 0.8914572864321608\n",
      "10 th training end\n",
      "10 th test accuracuy: 0.8839195979899498\n",
      "11 th training end\n",
      "11 th test accuracuy: 0.893467336683417\n",
      "12 th training end\n",
      "12 th test accuracuy: 0.8693467336683417\n",
      "13 th training end\n",
      "13 th test accuracuy: 0.892964824120603\n",
      "14 th training end\n",
      "14 th test accuracuy: 0.8984924623115578\n",
      "15 th training end\n",
      "15 th test accuracuy: 0.8874371859296483\n",
      "16 th training end\n",
      "16 th test accuracuy: 0.8547738693467337\n",
      "17 th training end\n",
      "17 th test accuracuy: 0.8804020100502512\n",
      "18 th training end\n",
      "18 th test accuracuy: 0.8753768844221106\n",
      "19 th training end\n",
      "19 th test accuracuy: 0.8864321608040201\n",
      "20 th training end\n",
      "20 th test accuracuy: 0.8703517587939699\n"
     ]
    }
   ],
   "source": [
    "TREE = data.Field(sequential=False,use_vocab=False,preprocessing=fun,lower=True)\n",
    "#NEST_TEXT = data.NestedField(TEXT,tokenize=split_n)\n",
    "NEST_TREE = data.NestedField(TREE,tokenize=split_n,use_vocab=False)\n",
    "TARGET = data.Field(sequential=False)\n",
    "airplane_train = data.TabularDataset(path='../finaldataset1/airplane_train.csv',format='csv',fields=[('nest_TREE',NEST_TREE),\n",
    "                                                                   ('target',TARGET)])\n",
    "airplane_test = data.TabularDataset(path='../finaldataset1/airplane_test.csv',format='csv',fields=[('nest_TREE',NEST_TREE),\n",
    "                                                                   ('target',TARGET)])\n",
    "\n",
    "print('The number of pairs of articles we have for the training set:', len(airplane_train.examples)/2)\n",
    "print('The number of pairs of articles we have for the testing set:' ,len(airplane_test.examples)/2)\n",
    "\n",
    "treeRnnAndGru = TreeRnnAndGRU(p1=0.5)\n",
    "treeRnnAndGru = treeRnnAndGru.cuda()\n",
    "optimizer = torch.optim.Adam(treeRnnAndGru.parameters(),lr=0.001,weight_decay=0)\n",
    "for epoch in range(20):\n",
    "    fit(epoch,treeRnnAndGru,airplane_train.examples,group=120)\n",
    "    pred(epoch,treeRnnAndGru,airplane_test.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best accuracy for earthquake is <b>0.9965</b>\n",
    "\n",
    "Best accuracy for airplane accident is <b>0.898</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some other famous models performance on the same dataset\n",
    "\n",
    "\n",
    "<img src=\"./result.png\" height=\"800\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../finaldataset1/'\n",
    "save_path = '../recursive data/'\n",
    "for name in os.listdir(path):\n",
    "    if re.findall('^[a-z0-9]',name):\n",
    "        df = pd.read_csv(path+name ,header=None)\n",
    "        df.columns = ['binary','target']\n",
    "        df['binary'] = df['binary'].apply(lambda x: x.replace('(','').replace(')',''))\n",
    "        df.to_csv(save_path+name,header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnAndGRU(nn.Module):\n",
    "    def __init__(self,p1=0.2):\n",
    "        super().__init__()\n",
    "        stoi = open('../vocabulary/stoi.pickle','rb')\n",
    "        self.stoi = pickle.load(stoi)\n",
    "        embedding = torch.load('../vocabulary/embedding')\n",
    "        self.embedding = embedding.cuda()\n",
    "        h_0 = torch.zeros(1,1,300)\n",
    "        h_0 = torch.normal(h_0,std=0.0001)\n",
    "        self.h_0 = nn.Parameter(h_0,requires_grad=True)\n",
    "        h_rnn = torch.zeros(1,300)\n",
    "        h_rnn = torch.normal(h_rnn,std=0.0001)\n",
    "        self.h_rnn = nn.Parameter(h_rnn,requires_grad=True)\n",
    "        \n",
    "        self.linear = nn.Linear(600,300)\n",
    "        self.drop = nn.Dropout(p1)\n",
    "        self.gru = nn.GRU(input_size=300,hidden_size=300,num_layers=1,bias=True,batch_first=False)\n",
    "        self.prob = nn.Linear(300,1)\n",
    "    def forward(self,example):\n",
    "        article_vector = None\n",
    "        for sentence in example.sentences:\n",
    "            h = self.h_rnn\n",
    "            for token in sentence:\n",
    "                e_t = self.embedding[self.stoi[token]].view(-1,300)\n",
    "                h = nn.functional.tanh(self.linear(torch.cat((e_t,h),dim=1)))\n",
    "            if article_vector is None:\n",
    "                article_vector = h.clone()\n",
    "            else:\n",
    "                article_vector = torch.cat((article_vector,h),dim=0)\n",
    "        seq,input_size = article_vector.shape\n",
    "        output,h_n = self.gru(article_vector.view(seq,1,input_size),self.h_0)\n",
    "        h_n = h_n.view(-1)\n",
    "        out = self.prob(h_n)\n",
    "        return nn.functional.sigmoid(out)\n",
    "    \n",
    "def fit(epoch,model,examples,group = 100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    indices = np.arange(len(examples))\n",
    "    np.random.shuffle(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[::-1]\n",
    "    indices = np.array_split(indices,group)\n",
    "    \n",
    "    for index in indices:\n",
    "        loss = torch.FloatTensor([0]).cuda()\n",
    "        #loss = torch.FloatTensor([0])\n",
    "        for e in index:\n",
    "            example = examples[e]\n",
    "            target = torch.FloatTensor([int(example.target)]).cuda()\n",
    "            #target = torch.FloatTensor([int(example.target)])\n",
    "            prob = model(example)\n",
    "            loss += nn.functional.binary_cross_entropy(prob,target,size_average=False)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        loss = (1/len(index))*loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch+1,'th training end')\n",
    "    return\n",
    "\n",
    "    \n",
    "def pred(epoch,model,examples):\n",
    "    model.eval()\n",
    "    pred = np.array([])\n",
    "    for example in examples:\n",
    "        prob = model(example)\n",
    "        target = np.array([int(example.target)])\n",
    "        prob = np.array([prob.item()])\n",
    "        pred = np.append(pred,prob)\n",
    "    a = pred[::2]\n",
    "    b = pred[1::2]\n",
    "    correct_rate = ((a>b).sum())/len(a)\n",
    "    print(epoch+1,'th test accuracuy:',correct_rate)\n",
    "    return\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs of articles we have for the training set: 1960.0\n",
      "The number of pairs of articles we have for the testing set: 1990.0\n",
      "1 th training end\n",
      "1 th test accuracuy: 0.8246231155778895\n",
      "2 th training end\n",
      "2 th test accuracuy: 0.749748743718593\n",
      "3 th training end\n",
      "3 th test accuracuy: 0.8175879396984924\n",
      "4 th training end\n",
      "4 th test accuracuy: 0.7914572864321608\n",
      "5 th training end\n",
      "5 th test accuracuy: 0.828643216080402\n",
      "6 th training end\n",
      "6 th test accuracuy: 0.8417085427135679\n",
      "7 th training end\n",
      "7 th test accuracuy: 0.8301507537688442\n",
      "8 th training end\n",
      "8 th test accuracuy: 0.835678391959799\n",
      "9 th training end\n",
      "9 th test accuracuy: 0.8366834170854272\n",
      "10 th training end\n",
      "10 th test accuracuy: 0.8361809045226131\n",
      "11 th training end\n",
      "11 th test accuracuy: 0.8371859296482412\n",
      "12 th training end\n",
      "12 th test accuracuy: 0.8386934673366834\n",
      "13 th training end\n",
      "13 th test accuracuy: 0.8396984924623115\n",
      "14 th training end\n",
      "14 th test accuracuy: 0.8391959798994975\n",
      "15 th training end\n",
      "15 th test accuracuy: 0.8386934673366834\n",
      "16 th training end\n",
      "16 th test accuracuy: 0.8391959798994975\n",
      "17 th training end\n",
      "17 th test accuracuy: 0.8386934673366834\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8c0a0b4a2bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mairplane_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mairplane_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8ae56cb8116a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epoch, model, examples, group)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'th training end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 88\u001b[0;31m         tensors, grad_tensors, retain_graph, create_graph)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentence = data.Field(sequential=True,use_vocab=False,lower=True)\n",
    "#NEST_TEXT = data.NestedField(TEXT,tokenize=split_n)\n",
    "sentences = data.NestedField(sentence,tokenize=split_n,use_vocab=False)\n",
    "TARGET = data.Field(sequential=False)\n",
    "airplane_train = data.TabularDataset(path='../recursive data/airplane_train.csv',format='csv',fields=[('sentences',sentences),\n",
    "                                                                   ('target',TARGET)])\n",
    "airplane_test = data.TabularDataset(path='../recursive data/airplane_test.csv',format='csv',fields=[('sentences',sentences),\n",
    "                                                                   ('target',TARGET)])\n",
    "\n",
    "print('The number of pairs of articles we have for the training set:', len(airplane_train.examples)/2)\n",
    "print('The number of pairs of articles we have for the testing set:' ,len(airplane_test.examples)/2)\n",
    "\n",
    "\n",
    "rnnAndGRU = RnnAndGRU(p1=0.5)\n",
    "rnnAndGRU = rnnAndGRU.cuda()\n",
    "optimizer = torch.optim.Adam(rnnAndGRU.parameters(),lr=0.001,weight_decay=0)\n",
    "for epoch in range(20):\n",
    "    fit(epoch,rnnAndGRU,airplane_train.examples,group=120)\n",
    "    pred(epoch,rnnAndGRU,airplane_test.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs of articles we have for the training set: 1490.0\n",
      "The number of pairs of articles we have for the testing set: 1720.0\n",
      "1 th training end\n",
      "1 th test accuracuy: 0.7854651162790698\n",
      "2 th training end\n",
      "2 th test accuracuy: 0.8517441860465116\n",
      "3 th training end\n",
      "3 th test accuracuy: 0.9430232558139535\n",
      "4 th training end\n",
      "4 th test accuracuy: 0.7343023255813953\n",
      "5 th training end\n",
      "5 th test accuracuy: 0.7517441860465116\n",
      "6 th training end\n",
      "6 th test accuracuy: 0.7261627906976744\n",
      "7 th training end\n",
      "7 th test accuracuy: 0.7546511627906977\n",
      "8 th training end\n",
      "8 th test accuracuy: 0.7825581395348837\n",
      "9 th training end\n",
      "9 th test accuracuy: 0.7610465116279069\n",
      "10 th training end\n",
      "10 th test accuracuy: 0.7325581395348837\n",
      "11 th training end\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-85c4daabd2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearthquake_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearthquake_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-8ae56cb8116a>\u001b[0m in \u001b[0;36mpred\u001b[0;34m(epoch, model, examples)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8ae56cb8116a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0me_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentence = data.Field(sequential=True,use_vocab=False,lower=True)\n",
    "#NEST_TEXT = data.NestedField(TEXT,tokenize=split_n)\n",
    "sentences = data.NestedField(sentence,tokenize=split_n,use_vocab=False)\n",
    "TARGET = data.Field(sequential=False)\n",
    "\n",
    "earthquake_train = data.TabularDataset(path='../recursive data/earthquake_train.csv',format='csv',fields=[('sentences',sentences),\n",
    "                                                                   ('target',TARGET)])\n",
    "earthquake_test = data.TabularDataset(path='../recursive data/earthquake_test.csv',format='csv',fields=[('sentences',sentences),\n",
    "                                                                   ('target',TARGET)])\n",
    "\n",
    "print('The number of pairs of articles we have for the training set:', len(earthquake_train.examples)/2)\n",
    "print('The number of pairs of articles we have for the testing set:' ,len(earthquake_test.examples)/2)\n",
    "\n",
    "rnnAndGRU = RnnAndGRU(p1=0.5)\n",
    "rnnAndGRU = rnnAndGRU.cuda()\n",
    "optimizer = torch.optim.Adam(rnnAndGRU.parameters(),lr=0.001,weight_decay=0)\n",
    "for epoch in range(20):\n",
    "    fit(epoch,rnnAndGRU,earthquake_train.examples,group=90)\n",
    "    pred(epoch,rnnAndGRU,earthquake_test.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs of articles we have for the training set: 1490.0\n",
      "The number of pairs of articles we have for the testing set: 1720.0\n",
      "1 th training end\n",
      "1 th test accuracuy: 0.7906976744186046\n",
      "2 th training end\n",
      "2 th test accuracuy: 0.9436046511627907\n",
      "3 th training end\n",
      "3 th test accuracuy: 0.8941860465116279\n",
      "4 th training end\n",
      "4 th test accuracuy: 0.8372093023255814\n",
      "5 th training end\n",
      "5 th test accuracuy: 0.852906976744186\n",
      "6 th training end\n",
      "6 th test accuracuy: 0.7715116279069767\n",
      "7 th training end\n",
      "7 th test accuracuy: 0.7604651162790698\n",
      "8 th training end\n",
      "8 th test accuracuy: 0.7761627906976745\n",
      "9 th training end\n",
      "9 th test accuracuy: 0.7854651162790698\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-85c4daabd2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearthquake_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnnAndGRU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearthquake_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-30af31e6dab2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epoch, model, examples, group)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'th training end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 88\u001b[0;31m         tensors, grad_tensors, retain_graph, create_graph)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentence = data.Field(sequential=True,use_vocab=False,lower=True)\n",
    "#NEST_TEXT = data.NestedField(TEXT,tokenize=split_n)\n",
    "sentences = data.NestedField(sentence,tokenize=split_n,use_vocab=False)\n",
    "TARGET = data.Field(sequential=False)\n",
    "\n",
    "earthquake_train = data.TabularDataset(path='../recursive data/earthquake_train.csv',format='csv',fields=[('sentences',sentences),\n",
    "                                                                   ('target',TARGET)])\n",
    "earthquake_test = data.TabularDataset(path='../recursive data/earthquake_test.csv',format='csv',fields=[('sentences',sentences),\n",
    "                                                                   ('target',TARGET)])\n",
    "\n",
    "print('The number of pairs of articles we have for the training set:', len(earthquake_train.examples)/2)\n",
    "print('The number of pairs of articles we have for the testing set:' ,len(earthquake_test.examples)/2)\n",
    "\n",
    "rnnAndGRU = RnnAndGRU(p1=0.5)\n",
    "rnnAndGRU = rnnAndGRU.cuda()\n",
    "optimizer = torch.optim.Adam(rnnAndGRU.parameters(),lr=0.001,weight_decay=0)\n",
    "for epoch in range(20):\n",
    "    fit(epoch,rnnAndGRU,earthquake_train.examples,group=90)\n",
    "    pred(epoch,rnnAndGRU,earthquake_test.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
